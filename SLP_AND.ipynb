
import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt

# STEP 1. Define the model
class Perceptron(nn.Module):
    def __init__(self):
        super(Perceptron, self).__init__()
        self.linear = nn.Linear(2, 1)

    def forward(self, x):
        return torch.sigmoid(self.linear(x))

# STEP 2. Define the training data
# AND operation
x_train = torch.FloatTensor([
    [0, 0], 
    [0, 1], 
    [1, 0], 
    [1, 1]
])
y_train = torch.FloatTensor([
    [0], 
    [0], 
    [0], 
    [1]
])

x_test = torch.FloatTensor([[0, 0], [0, 1], [1, 0], [1, 1]])

# Initialize the model
model = Perceptron()

# STEP 3. Define the loss function and optimizer
loss_function = nn.BCELoss()
optimizer_function = optim.SGD(model.parameters(), lr=0.1)

# STEP 4. Train the model
losses = []
for epoch in range(10000):
    # Forward pass
    y_output = model(x_train)

    # Compute the loss
    loss = loss_function(y_output, y_train)

  # Store the loss
    losses.append(loss.item())


    # Backward pass
    optimizer_function.zero_grad()
    loss.backward()

    # Update the parameters
    optimizer_function.step()


# RESULT
# Plot the losses
plt.plot(losses)
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.show()


# STEP6. Test the model
with torch.no_grad():
    for i in range(4):
        print(f"Input: {x_test[i]}, Output: {model(x_test[i])}")



